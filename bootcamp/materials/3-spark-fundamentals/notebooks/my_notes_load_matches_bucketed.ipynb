{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f14381ed-8da4-4748-b1d1-03136979c65c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://88fd5f590760:4042\n",
       "SparkContext available as 'sc' (version = 3.5.1, master = local[*], app id = local-1733674486493)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.DataFrame = []\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP table bootcamp.matches_bucketed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9de6d13b-d7e4-4065-a0dd-27f4c8621d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+-------------------+\n",
      "|            match_id|is_team_game|         playlist_id|    completion_date|\n",
      "+--------------------+------------+--------------------+-------------------+\n",
      "|0df7e36f-9501-483...|        true|2323b76a-db98-4e0...|2016-08-07 00:00:00|\n",
      "|a582acd7-aea5-419...|        NULL|f72e0ef0-7c4a-430...|2015-12-26 00:00:00|\n",
      "|7d2b104b-af02-49b...|        NULL|f72e0ef0-7c4a-430...|2015-12-26 00:00:00|\n",
      "|fe41a901-7afe-408...|        NULL|2323b76a-db98-4e0...|2015-12-26 00:00:00|\n",
      "|0e05752a-10f2-493...|        true|bc0f8ad6-31e6-4a1...|2015-12-26 00:00:00|\n",
      "|ceeeefd4-ce81-49e...|        NULL|2323b76a-db98-4e0...|2015-12-26 00:00:00|\n",
      "|d7a45423-226b-47a...|        NULL|d0766624-dbd7-453...|2015-12-26 00:00:00|\n",
      "|7d72b72e-3864-403...|        NULL|f72e0ef0-7c4a-430...|2015-12-26 00:00:00|\n",
      "|340905d8-f5ce-45c...|        true|bc0f8ad6-31e6-4a1...|2015-12-26 00:00:00|\n",
      "|6e49636a-e9d1-4f1...|        true|2323b76a-db98-4e0...|2016-08-07 00:00:00|\n",
      "|45cd2847-3773-414...|        NULL|d0766624-dbd7-453...|2015-12-26 00:00:00|\n",
      "|2c4821c4-be79-4b5...|        true|f72e0ef0-7c4a-430...|2015-12-26 00:00:00|\n",
      "|be540db9-163e-47a...|        true|f72e0ef0-7c4a-430...|2015-12-26 00:00:00|\n",
      "|00592b3d-ae26-45b...|        NULL|f72e0ef0-7c4a-430...|2015-12-26 00:00:00|\n",
      "|ab7d172a-ede0-4b2...|        NULL|f72e0ef0-7c4a-430...|2015-12-26 00:00:00|\n",
      "|97c7e979-dcbc-48f...|        NULL|f72e0ef0-7c4a-430...|2015-12-26 00:00:00|\n",
      "|7e0388b2-1792-413...|        true|f72e0ef0-7c4a-430...|2015-12-26 00:00:00|\n",
      "|a87d7073-3b84-42c...|       false|d0766624-dbd7-453...|2015-12-26 00:00:00|\n",
      "|3e4fd2b3-8ab0-4d9...|        NULL|2323b76a-db98-4e0...|2015-12-26 00:00:00|\n",
      "|71e1391c-f6f7-436...|        NULL|f72e0ef0-7c4a-430...|2015-12-26 00:00:00|\n",
      "+--------------------+------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.sql.functions.col\n",
       "import org.apache.spark.storage.StorageLevel\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@7e56f5e9\n",
       "matchesBucketedselect: org.apache.spark.sql.DataFrame = [match_id: string, mapid: string ... 8 more fields]\n",
       "distinctDates: Array[org.apache.spark.sql.Row] = Array([2016-03-13 00:00:00.0], [2016-03-11 00:00:00.0], [2016-03-10 00:00:00.0], [2016-01-30 00:00:00.0], [2016-03-27 00:00:00.0], [2016-04-10 00:00:00.0], [2016-01-18 00:00:00.0], [2016-02-01 00:00:00.0], [2015-12-14 00:00:00.0], [2016-02-03 00:00:00.0], [2016-04-30 00:00:00.0], [2016-03-05 00:00:00.0], [2016-04-15 00:00:00.0], [2016-05-21 00:00:00.0], [2015-10-31 00:00:00.0], [2016-01-22 00:00:00.0], [2016-02-09 00:00:00...\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions.{col}\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"IcebergTableManagement\") \n",
    "  .config(\"spark.executor.memory\", \"4g\")\n",
    "  .config(\"spark.driver.memory\", \"4g\")\n",
    "  .config(\"spark.sql.shuffle.partitions\", \"200\") // Fine for large datasets\n",
    "  .config(\"spark.sql.files.maxPartitionBytes\", \"134217728\") // Optional: 128 MB is default\n",
    "  .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") // Optional: Disable broadcast join\n",
    "  .config(\"spark.dynamicAllocation.enabled\", \"true\") // Helps with resource allocation\n",
    "  .config(\"spark.dynamicAllocation.minExecutors\", \"1\") // Ensure minimum resources\n",
    "  .config(\"spark.dynamicAllocation.maxExecutors\", \"50\") // Scalable resource allocation\n",
    "  .getOrCreate()\n",
    "\n",
    "\n",
    "val matchesBucketedselect = spark.read.option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .csv(\"/home/iceberg/data/matches.csv\")\n",
    "\n",
    "// Get distinct completion dates\n",
    "val distinctDates = matchesBucketed.select(\"completion_date\").distinct().collect()\n",
    "\n",
    "// Create the Iceberg table if it doesn't exist\n",
    "val bucketedDDL = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bootcamp.matches_bucketed (\n",
    "    match_id STRING,\n",
    "    is_team_game BOOLEAN,\n",
    "    playlist_id STRING,\n",
    "    completion_date TIMESTAMP\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (completion_date, bucket(16, match_id))\n",
    "\"\"\"\n",
    "spark.sql(bucketedDDL)\n",
    "\n",
    "// Process data in chunks based on completion_date\n",
    "distinctDates.foreach { row =>\n",
    "  val date = row.getAs[java.sql.Timestamp](\"completion_date\")\n",
    "  val filteredMatches = matchesBucketed.filter(col(\"completion_date\") === date)\n",
    "  \n",
    "  // Repartition and persist the filtered data\n",
    "  val optimizedMatches = filteredMatches\n",
    "    .select($\"match_id\", $\"is_team_game\", $\"playlist_id\", $\"completion_date\")\n",
    "    .repartition(16, $\"match_id\")\n",
    "    .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    \n",
    "  optimizedMatches.write\n",
    "    .mode(\"append\")\n",
    "    .bucketBy(16, \"match_id\")\n",
    "    .partitionBy(\"completion_date\")\n",
    "    .saveAsTable(\"bootcamp.matches_bucketed\")\n",
    "}\n",
    "\n",
    "// Verify the data in the table\n",
    "val result = spark.sql(\"SELECT * FROM bootcamp.matches_bucketed\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5325e4c-4322-40a1-af7c-e5cf71950901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|num_files|\n",
      "+---------+\n",
      "|     3665|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(1) as num_files FROM bootcamp.matches_bucketed.files\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6fdb50-a750-43e0-890e-95f4e19a7c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
